---
title: 激活函数
date: 2021-9-21
tags: 深度学习基础
---

定义（What）：**上层节点的输出和下层节点的输入**之间具有一个函数关系，这个函数称为激活函数

作用（Why）：引入**非线性函数**作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是**几乎可以逼近任意函数**）

选择方法（How）：

1.尽量选择zero-centered输出的激活函数，如tanh

2.使用ReLU （ max（0，x））时，小心设置lr，避免出现死神经元，也可以用

leaky relu（max（ax，x），ax=0.01）、prelu或maxout

### 种类

## 1.Sigmoid函数（映射到0—1）

**优点**：它能够把输入的连续实值变换为0和1之间的输出

**缺点**：在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失，其中梯度爆炸发生的概率非常小，**而梯度消失发生的概率比较大**。

## 2.ReLU

**优点**：正区间解决梯度消失问题、计算快、收敛快于sigmoid、tanh

**缺点**：输出不是zero-centered、Dead ReLU Problem，指的是某些神经元可能永远不会被激活

避免死神经元，可以用**Xavier初始化**方法，以及避免将learning rate设置太大或使用**adagrad**等自动调节learning rate的算法

## 3.tanh

**优点**：解决了Sigmoid函数的不是zero-centered输出问题

**缺点**：梯度消失（gradient vanishing）的问题和幂运算的问题仍然存在

参考文献：[常用激活函数（激励函数）理解与总结_tyhj_sf的博客空间-CSDN博客_激活函数](https://blog.csdn.net/tyhj_sf/article/details/79932893)

